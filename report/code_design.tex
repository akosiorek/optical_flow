\section{Code Design}
Summary of needs of the algorithm when looking at actual implementations not just mathematical description.
\vspace*{2cm} \\ 

Computing truly event-based optical flow would require the algorithm to operate on discerete events, evaluating the contribution of each and every one of them to optical flow field generated by all other ones.
Memory and computational power power required render it infeasible.
We make the following simplifications.
Firstly, we discretize the spatio-temporal Gabor filter in time with a given time window and in space with a resolution equal to that of the DVS camera.
Tempoeral and spatial span of the filter is big enough to incorporate all points on which the magnitude of filter's response is higher than some threshold value.
The filter thus comprises of discerte time-slices, which we call Filter Slices (FS).
Secondly, all events occuring in a single time window are grouped together or quantized into the so-called Event Slice (ES) and are assumed to have occured at the same moment, \emph{i.e.} the beginning of the time window. 
An ES is a square matrix, with the number of elements equal to that of the DVS camera. 
With this setup, computing optical flow means filtering the incoming stream of Event Slices with the previously setup filter.
A natural approach is to streamline the work, setting several components up in a pipeline.
Data comes into the pipeline and is processed sequentially by each of the compnents, leaving as a final product or optical flow in our case. 
Typically, data has to be read and post-processed.
This gives the final shape to our pipeline, as shown in figure \ref{fig:pipeline}, which consists of EventReader, Quantizer, FilteringEngine and FlowSink objects.
Since performance is our top priority, we aim for asynchronous execution of the steps with possibly parallel execution inside each of them.
This approach requires buffering of data between components, which we realize with blocking unbounded buffers. 
Abstracting interfaces of each component enforces modularity and allows changing building blocks easily.

\subsection{Overview Pipeline}
As our previous class diagram, as flow chart perhaps
\vspace*{2cm} \\ 

EventReader reads an event stream from different sources \emph{e.g.} text file, binary file or directly from a DVS camera. 
Quantizer quantizes discrete events into Event Slices.
FilteringEngines consumes EventSlices. It has an internal buffer, which holds enough events to cover the whole time span of the filter. Once the buffer fills up, FilteringEngine starts to produce Flow Slices.
FlowSink handles Flow Slices. It can \emph{e.g.} serialize or visualize them.

\subsection{Filtering Engine}

% introduction
Filtering in spatio-temporal domain is applied to data by convolving it linearly with the filter.
The filter, discretized as above, as well as Event Slices corresponding to the filter's time span can be seen as big 3-dimensional matrices.
It is well known that once the matrix' sizes are big enough, filtering in spatio-temporal domain can be prohibitatively expensive.
We could either exploit sparse structure of Event Slices or transform problem to the frequency domain, where spatio-temporal convolution corresponds to element-wise multiplication of the filter with data.
We decided to go with the 2nd approach.
\\

% why series of 2D convolutions
Possibly the most efficient way to carry out convolution would be to aggregate enough Event Slices, apply 3D Fourier transform, multiply it with the transformed filter element-wise and apply the 3D inverse transform to the result.
There are 2 limiting factors, however.
Firstly, linear convolution requires padding both the filter and data to the same size, in 1D given by $M+N-1$ where $M$ -- filter dimension and $N$ -- data dimension.
It significantly increases the amount of elements to process, the more the higher the dimensionality.
Secondly, at every time step (for every incoming Event Slice) the process of padding and applying 3D Fourier transform would have to be repeated.
Each ES would have to be transformed as many times as many Filter Slices there are.
Therefore, we decided to use 2-dimensional Fourier transforms, which enable vast reuse of values computed in previous iterations of the algorithm.

% details of 2D conv
3-dimensional convolution, as a linear operation, can be expressed as a sum of 2-dimensional ones.
Each Event Slice still has to be convolved with all Filter Slices, but it can be transformed to the frequency domain only once and reused later.
Since following Event Slices do not have to be contigious in memory (as it would be the case with 3D Fourier convolution) we can reuse it by storing Event Slices in a circular buffer.
When a new Event Slice comes in, the buffer is rotated and the last Event Slice, which isn't needed any more, is overwritten with the new one.

% modularity
To further increase modularity, Fourier-convolution based FilteringEngines use three objects via dependency injection: FilterFactory, which produces filters of for given angle, FourierPadder, which padds data and filters to size required by Fourier-based linear convolution, and FourierTransformer, which carries out forward and backward Fourier transform.

\begin{algorithm}
 \caption{Filtering Algorithm}
 \label{algo:optic}
 \begin{algorithmic}[1]
  \Procedure{Filter}{eventSlice}
  \State rotate(eventSliceBuffer)
  \State pad(eventSlice)  
  \State slice(eventSliceBuffer, 0) = forwardFourier(eventSlice)
  \State flow = 0
  \For{each filter $\in$ filters }
  \State response = 0
  \For{each $i \in$ numSlices(filter) }
  \State filterSlice = slice(filter, $i$)
  \State eventSlice = slice(eventSliceBuffer, $i$)
  \State response += filterSlice $\cdot$ eventSlice
  \EndFor
  \State flow.X += cos(angle(filter)) $\cdot$ response
  \State flow.Y -= sin(angle(filter)) $\cdot$ response
  \EndFor
  \State inverseFourier(flow.X)
  \State inverseFourier(flow.Y)
  \State return flow
  \EndProcedure
\end{algorithmic}

\end{algorithm}


\subsection{Other components}
Other Stuff??