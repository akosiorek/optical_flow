% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Article{brandli2014240,
  Title                    = {A 240$\times$ 180 130 dB 3 $\mu$s Latency Global Shutter Spatiotemporal Vision Sensor},
  Author                   = {Brandli, Christian and Berner, Raphael and Yang, Minhao and Liu, Shih-Chii and Delbruck, Tobi},
  Journal                  = {Solid-State Circuits, IEEE Journal of},
  Year                     = {2014},
  Number                   = {10},
  Pages                    = {2333--2341},
  Volume                   = {49},

  Owner                    = {johannes},
  Publisher                = {IEEE},
  Timestamp                = {2015.07.03}
}

@Article{Brosch2015,
  Title                    = {On event-based optical flow detection},
  Author                   = {Brosch, Tobias and Tschechne, Stephan and Neumann, Heiko},
  Journal                  = {Frontiers in Neuroscience},
  Year                     = {2015},
  Number                   = {137},
  Volume                   = {9},

  Abstract                 = {Event-based sensing, i.e. the asynchronous detection of luminance changes, promises low-energy, high dynamic range, and sparse sensing. This stands in contrast to whole image frame-wise acquisition by standard cameras. Here, we systematically investigate the implications of event-based sensing in the context of visual motion, or flow, estimation. Starting from a common theoretical foundation, we discuss different principal approaches for optical flow detection ranging from gradient-based methods over plane-fitting to filter based methods and identify strengths and weaknesses of each class. Gradient-based methods for local motion integration are shown to suffer from the sparse encoding in address-event representations (AER). Approaches exploiting the local plane like structure of the event cloud, on the other hand, are shown to be well suited. Within this class, filter based approaches are shown to define a proper detection scheme which can also deal with the problem of representing multiple motions at a single location (motion transparency). A novel biologically inspired efficient motion detector is proposed, analyzed and experimentally validated. Furthermore, a stage of surround normalization is incorporated. Together with the filtering this defines a canonical circuit for motion feature detection. The theoretical analysis shows that such an integrated circuit reduces motion ambiguity in addition to decorrelating the representation of motion related activations.},
  Doi                      = {10.3389/fnins.2015.00137},
  ISSN                     = {1662-453X},
  Url                      = {http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2015.00137/abstract}
}

@Article{Delbruck,
  Title                    = {A 128 2128 120 dB 30 mW asynchronous vision sensor that responds to relative intensity change},
  Author                   = {Delbruck, T and Lichtsteiner, P},
  Journal                  = {IEEE ISSCC Dig. Tech. Papers},
  Year                     = {2005}
}

@MastersThesis{Scherer2015,
  Title                    = {Datasets and a unified systems for the evaluation of optic flow from event based data},
  Author                   = {Florian Scherer},
  School                   = {Technical University of Munich},
  Year                     = {2015},
  Note                     = {unpublished},

  Owner                    = {johannes},
  Timestamp                = {2015.07.03}
}

@InProceedings{weikersdorfer2014event,
  Title                    = {Event-based 3D SLAM with a depth-augmented dynamic vision sensor},
  Author                   = {Weikersdorfer, David and Adrian, David B and Cremers, Daniel and Conradt, Jorg},
  Booktitle                = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {359--364}
}

